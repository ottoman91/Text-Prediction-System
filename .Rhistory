inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
head(training)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
head(training)
head(concrete)
xnames <- colnames(concrete)[1:8]
featurePlot(X=training[,xnames],y=training$CompressiveStrength,plot="pairs")
featurePlot(x=training[,xnames],y=training$CompressiveStrength,plot="pairs")
library(Hmisc)
cutCompressiveStrength <- cut2(training$CompressiveStrength,g=4)
table(cutCompressiveStrength)
hist(training$Superplasticizer)
set.seed(3433)data(AlzheimerDisease)
set.seed(3433);data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
head(training)
ncol(training)
newtraining <- training[,55:66]
data(newtraining)
head(newtraining)
newtraining <- newtraining[,4:12]
head(newtraining)
ncol(newtraining)
preProc <- preProcess(log10(newtraining+1),method="pca",pcaComp=2)
preProc <- preProcess(log10(newtraining+1),method="knnImpute",pcaComp=2)
ss <- training[,grep('^IL',x=names(training))]
head(ss)
preProc <- preProcess(ss,method='pca',thresh=0.9,outcome=training$diagnosis)
preProc$rotation
preProc$rotation
data("iris")
library(ggplot2)
name(iris)
names(iris)
table(iris$Species)
library(caret)
inTrain <- createDataPartition(y=iris$Species,p=0.7,list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training)
dim(testing)
names(iris)
qplot(Petal.Width, Sepal.Width,colour=Species,data=training)
library(caret)
modFit <- train(Species ~ .,method="rpart",data=training)
print(modFit$finalModel)
plot(modFit$finalModel,uniform=TRUE, main="Classification Tree")
text(modFit$finalModel,use.n = TRUE, all = TRUE, cex=.8)
install.packages("rattle")
library(rattle)
library(rattle)
library(rattle)
data("iris")
library(ggplot2)
library(caret)
inTrain <- createDataPartition(y=iris$Species,p=0.7,list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
modFit <- train(Species ~ ., method="rpart", data=training)
fancyRpartPlot(modFit$finalModel)
library(rattle)
library(rattle)
library(ElemStatLearn)
install.packages("ElemStatLearn")
library(ElemStatLearn)
data("ozone",package="ElemStatLearn")
ozone <- ozone[order(ozone$ozone),]
head(ozone)
ll <- matrix(NA,nrow = 10,ncol=155)
for(i in 1:10){
ss <- sample(1:dim(ozone)[1],replace = T)
ozone0 <- ozone[ss,]
ozone0 <- ozone[order(ozone0$ozone),]
loess0 <- loess(temperature ~ ozone, data=ozone0,span = 0.2)
ll[i,] <- predict(loess0,newdata=data.frame(ozone=1:155))
}
plot(ozone$ozone,ozone$temperature,pch=19,cex=0.5)
for(i in 1:10){lines(1:155,ll[i,],col="grey",lwd=2)}
lines(1:155,apply(ll,2,mean),col="red",lwd=2)
ll <- matrix(NA,nrow=10,ncol=155)
for(i in 1:10){
ss <- sample(1:dim(ozone)[1],replace=T)
ozone0 <- ozone[ss,]; ozone0 <- ozone0[order(ozone0$ozone),]
loess0 <- loess(temperature ~ ozone,data=ozone0,span=0.2)
ll[i,] <- predict(loess0,newdata=data.frame(ozone=1:155))
}
ll <- matrix(NA,nrow=10,ncol=155)
for(i in 1:10){
ss <- sample(1:dim(ozone)[1],replace=T)
ozone0 <- ozone[ss,]; ozone0 <- ozone0[order(ozone0$ozone),]
loess0 <- loess(temperature ~ ozone,data=ozone0,span=0.2)
ll[i,] <- predict(loess0,newdata=data.frame(ozone=1:155))
}
plot(ozone$ozone,ozone$temperature,pch=19,cex=0.5)
for(i in 1:10){lines(1:155,ll[i,],col="grey",lwd=2)}
lines(1:155,apply(ll,2,mean),col="red",lwd=2)
for(i in 1:10){lines(1:155,ll[i,],col="grey",lwd=2)}
lines(1:155,apply(ll,2,mean),col="red",lwd=2)
library(iris)
data(iris)
install.packages("rattle")
library(rattle)
library(rattle)
library(caret)
library(ggplot2)
data(iris)
inTrain <- createDataPartition(y=iris$Species, p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
modFit <- train(Species ~ ., data=training,method="rf",prox=TRUE)
modFit
getTree(modFit$finalModel,k=2)
head(training)
irisP <- classCenter(training[,c(3,4)],training$Species,modFit$finalModel$prox)
irisP <- as.data.frame(irisP)
irisP$Species <- rownames(irisP)
p <- qplot(Petal.Width, Petal.Length, col=Species, data=training)
p
p + geom_point(aes(x=Petal.Width,y=Petal.Length,col=Species),size=5,shape=4,data=irisP)
p
p + geom_point(aes(x=Petal.Width,y=Petal.Length,col=Species),size=5,shape=4,data=irisP)
pred <- predict(modFit, testing)
testing$predRight <- pred==testing$Species
table(pred,testing$Species)
qplot(Petal.Width,Petal.Length, colour=predRight, data=testing, main="newdata Predictions")
library(ISLR)
data(Wage)
Wage <- subset(Wage, select=-c(logwage))
inTrain <- createDataPartition(y=Wage$wage, p = 0.7, list=FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
modFit <- train(wage ~ ., method="gbm",data=training,verbose=FALSE)
warnings()
modFit <- train(wage ~ ., method="gbm",data=training,verbose=FALSE)
print(modFit)
qplot(predict(modFit,testing),wage,data=testing)
names(iris)
table(iris$Species)
inTrain <- createDataPartition(y=iris$Species,p=0.7,list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training);dim(testing)
modlda <- train(Species ~ ., data=training, method="lda")
modnb <- train(Species ~ ., data=training, method="nb")
plda = predict(modlda,testing)
pnb = predict(modnb,testing)
table(plda,pnb)
equalPredictions == (plda==pnb)
equalPredictions = (plda==pnb)
qplot(Petal.Width,Sepal.Width,colour=equalPredictions,data=testing)
library(rattle)
data(iris)
inTrain <- createDataPartition(y=iris$Species,p=0.7, list=FALSE)
library(caret)
inTrain <- createDataPartition(y=iris$Species,p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
modFit <- train(Species ~ .,method="rpart",data=training)
fancyRpartPlot(modFit$finalModel)
fancyRpartPlot(modFit$finalModel)
library(rpart.plot)
library(rpart)
fancyRpartPlot(modFit$finalModel)
rattle::fancyRpartPlot(modFit$finalModel)
install.packages("rpart.plot")
rattle::fancyRpartPlot(modFit$finalModel)
predict(modFit,newdata=testing)
library(ElmStatLearn)
install.packages("ElmStatLearn")
library(ElemStatLearn)
data(ozone)
data(ozone,package="ElemStatLearn")
ozone <- ozone[order(ozone$ozone),]
ll <- matrix(NA,nrow=10,ncol=155)
library(party)
install.packages("party")
library(party)
data(ozone, package = "ElemStatLearn")
ozone <- ozone[order(ozone$ozone),]
predictors <- data.frame(ozone=ozone$ozone)
temperature <- ozone$temperature
treebag <- bag(predictors, temperature, B = 10,
# custom bagging function
bagControl = bagControl(fit = ctreeBag$fit,
predict = ctreeBag$pred,
aggregate = ctreeBag$aggregate))
plot(ozone$ozone,temperature,col="lightgrey,pch=19")
plot(ozone$ozone,temperature,col="lightgrey",pch=19)
points(ozone$ozone,predict(treebag$fits[[1]]$fit,predictors),pch=19,col="red")
points(ozone$ozone,predict(treebag,predictors),pch=19,col="blue")
data(iris)
inTrain <- createDataPartition(y=iris$Species,p=0.7,list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
modFit <- train(Species ~ .,data=training,method="rf"prox=TRUE)
modFit <- train(Species ~ .,data=training,method="rf",prox=TRUE)
head(getTree(modFit$finalModel,k=2))
irisP <- classCenter(training[,c(3,4)],training$Species,modFit$finalModel$prox)
p <- qplot(Petal.Width,Petal.Length,col=Species,data=training)
p + geom_point(aes(x=Petal.Width,y=Petal.Length,col=Species),size=5,shape=4,data=irisP)
geom_point(aes(x=Petal.Width,y=Petal.Length,col=Species),size=5,shape=4,data=irisP)
irisP <- classCenter(training[,c(3,4)], training$Species, modFit$finalModel$prox)
irisP <- as.data.frame(irisP); irisP$Species <- rownames(irisP)
p <- qplot(Petal.Width, Petal.Length, col=Species,data=training)
p + geom_point(aes(x=Petal.Width,y=Petal.Length,col=Species),size=5,shape=4,data=irisP)
pred <- predict(modFit,testing)
testing$predRight <- pred==testing$Species
table(pred,testing$Species)
data(Wage)
library(IRIS)
library(iris)
data(Wage)
library(ISLR)
data(Wage)
Wage <- subset(Wage,select=-c(logwage))
inTrain <- createDataPartition(y=Wage$wage,p=0.7,list=FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
modFit <- train(wage ~ .,method="gbm",data=training,verbose=FALSE)
print(modFit)
lda <- train(Species ~ .,data=training,method="lda")
data("iris")
inTrain <- createDataPartition(y=iris$Species,p=0.7,list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
lda <- train(Species ~ .,data=training,method="lda")
pred.lda <- predict(lda,testing)
pred.lda
nb <- train(Species ~ .,data=training, method="nb")
pred.bd <- predict(nb,testing)
pred.nb
pred.bd
table(pred.lda,pred.bd)
equalPredictions <- (pred.lda == pred.bd)
qplot(Petal.Width,Sepal.Width,colour=equalPredictions,data=testing)
library(AppliedPredictiveModeling)
library(caret)
data("segmentationOriginal")
set.seed(125)
names(segmentationOriginal)
inTrain <- createDataPartition(y=segmentationOriginal$Case,p=0.7,list=FALSE)
?rpart
library(pgmm)
install.packages(pgmm)
install.packages("pgmm")
library(pgmm)
data("olive")
names(olive)
olive <- olive[,-1]
inTrain <- createDataPartition(y=olive$Area,p=0.7,list=FALSE)
training <- olive[inTrain,]
testing <- olive[-inTrain,]
modFit <- train(Area ~ .,data=training,method="rpart")
print(modFit$finalModel)
newdata = as.data.frame(t(colMeans(olive)))
predict(modFit,newdata=newdata = as.data.frame(t(colMeans(olive)))
)
predict(modFit,newdata=newdata = as.data.frame(t(colMeans(olive)))
)
predict(modFit,newdata=newdata = as.data.frame(t(colMeans(olive))))
predict(modFit,newdata = as.data.frame(t(colMeans(olive))))
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
names(train)
names(trainSA)
modFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl,method="glm",data=trainSA,family="binomial")
modFit
testpredictions <- predict(modFit,testSA)
trainpredictions <- predict(modFit,trainSA)
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
misclassificationRateTestValues <- missClass(testSA,testpredictions)
misclassificationRateTrainValues <- missClass(trainSA,trainpredictions)
misclassificationRateTestValues
misclassificationRateTrainValues
modFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl,method="glm",data=trainSA,family="binomial")
head(trainSA)
dmy <- dummyVars("~ .",data=trainSA)
trainSA <- data.frame(predict(dmy,newdata=trainSA))
modFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl,method="glm",data=trainSA,family="binomial")
misclassificationRateTestValues <- missClass(testSA$chd,testpredictions)
misclassificationRateTrainValues <- missClass(trainSA$chd,trainpredictions)
misclassificationRateTrainValues
misclassificationRateTestValues
data(vowel.train)
data(vowel.test)
names(vowel.train)
vowel.test$y <- as.factor(vowel.test$y)
vowel.train$y <- as.factor(vowel.train$y)
set.seed(33833)
modFit <- train(y ~ .,method="rf",data=vowel.train,prox=TRUE)
?varImp
order(varImp(modFit),decreasing = T)
modFit <- randomForest(y ~ .,data=vowel.train,importance=FALSE)
order(varImp(modFit),decreasing = T)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
data <- segmentationOriginal
inTrain <- data$Case == "Train"
training <- data[inTrain,]
testing <- data[-inTrain,]
cartModel <- train(Class ~ .,method="rpart",data=training)
cartModel$finalModel
plot(cartModel$finalModel, uniform = T)
text(cartModel$finalModel, cex= 0.8)
library(rattle)
fancyRpartPlot(cartModel$finalModel)
require(caret)
require(data.frame)
trainUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
trainingData <- fread(trainUrl)
require(data.frame)
install.packages("data.frame")
require(data.table)
trainingData <- fread(trainUrl)
testingData <- fread(testUrl)
isAnyMissing <- sapply(testingData, function (x) any(is.na(x) | x == ""))
isPredictor <- !isAnyMissing & grepl("belt|[^(fore)]arm|dumbbell|forearm", names(isAnyMissing))
predCandidates <- names(isAnyMissing)[isPredictor]
varToInclude <- c("classe", predCandidates)
trainingData <- trainingData[,varToInclude, with=FALSE]
trainingData <- trainingData[,classe := factor(trainingData[,classe])]
inTrain <- createDataPartition(trainingData$classe,p=0.6,list=FALSE)
training <- trainingData[inTrain,]
testing <- trainingData[-inTrain,]
rf.fit <- randomForest(classe ~ .,data=training,trControl=trainControl(method = "cv"),number=10)
require(caret)
library(caret)
library(randomForest)
rf.fit <- randomForest(classe ~ .,data=training,trControl=trainControl(method = "cv"),number=10)
hat <- predict(rf.fit, testing)
confusionMatrix(hat,testing[,classe])
confusionMatrix(hat,testing[,classe])
library(shiny)
runApp()
install.packages("devtools")
devtools::install_github('rstudio/shinyapps')
library(shinyapps)
runApp()
runApp()
runApp()
x = 0
runApp()
runApp(display.mode = 'showcase')
runApp(display.mode = 'showcase')
runApp(display.mode = 'showcase')
runApp(display.mode = 'showcase')
runApp(display.mode = 'showcase')
install.packages("rCharts")
require(devtools)
install_github('rCharts', 'ramnathv')
require(rCharts)
haireye = as.data.frame(HairEyeColor)
nPlot(Freq ~ Hair, group = 'Eye', type = 'multiBarChart', data=subset(haireye, Sex == 'Male'))
install.packages("googleVis")
library(googleVis)
suppressPackageStartupMessages(library(googleVis))
M <- gvisMotionChart(Fruits, "Fruit", "Year", options=list(width = 600, height = 400))
print(M, 'chart')
plot(M)
demo(googleVis)
require(rCharts)
require(devtools)
install_github("ropensci/plotly")
library(manipulate)
myPlot <- function(s) {
plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
abline(0, s)
}
manipulate(myPlot(s), x.s = slider(0, 2, step = 0.1))
manipulate(myPlot, s = slider(0, 2, step = 0.1))
manipulate(myPlot, s = slider(0, 2, step = 0.1))
manipulate(myPlot(s), slider = x(0, 2, step = 0.1))
manipulate(myPlot(s), s = slider(0, 2, step = 0.1))
require(rCharts)
data("airquality")
dTable(airquality, sPaginationType = "full_numbers")
d <- data.frame(airquality, stringsAsFactors = FALSE)print(d)
install.packages("twitterR")
URL <- "https://d37djvu3ytnwxt.cloudfront.net/asset-v1:MITx+15.071x_3+1T2016+type@asset+block/climate_change.csv"
download.file(URL, destfile = 'climate_change.csv',method = 'wget')
dataset <- read.csv('climate_change.csv')
str(dataset)
training <- subset(dataset, Year < = 2006)
training <- subset(dataset, dataset$Year < = 2006)
training <- subset(dataset, dataset$Year <= 2006)
testing <- subset(dataset, dataset$Year > 2006)
?lm
model <- lm(Temp ~ MEI + CO2 + CH4 + N2O + CFC.11 + CFC.12 + TSI + Aerosols, data=training)
summary(model)
cor(training$N2O, training$MEI)
cor(training$N2O, training$CO2)
cor(training$N2O, training$CH4)
cor(training$N2O, training$CFC.11)
cor(training$N2O, training$CFC.12)
cor(training$N2O, training$Aerosols)
cor(training$N2O, training$TSI)
cor(training$CFC.11, training$MEI)
cor(training$CFC.11, training$CO2)
cor(training$CFC.11, training$CH4)
cor(training$CFC.11, training$N2O)
cor(training$CFC.11, training$CFC.12)
cor(training$CFC.11, training$Aerosols)
cor(training$CFC.11, training$TSI)
model2 <- lm(Temp ~ MEI + N2O + TSI + Aerosols, data=training)
summary(model2)
stepmodel <- step(model)
summary(stepmodel)
?predict
predict(testing, stepmodel)
predict(testing$Temp, stepmodel)
predict(stepmodel, testing$Temp)
predict(stepmodel, testing)
testing_set <- predict(stepmodel, testing)
summary(testing_set)
tempPredict = predict(stepmodel,newdata = testing)
SSE = sum((tempPredict - testing$Temp)^2)
SST = sum(mean(training$Temp - testing$Temp)^2)
1 - SSE/SST
url <-"www.cc.gatech.edu/~stasko/6750/Talks/06-reqts-gather.pdf"
download.file(url,destfile = "report.pdf", method="wget")
list.files()
getwd()
URL <- "https://d37djvu3ytnwxt.cloudfront.net/asset-v1:MITx+15.071x_3+1T2016+type@asset+block/pisa2009train.csv"
URL1 <- "https://d37djvu3ytnwxt.cloudfront.net/asset-v1:MITx+15.071x_3+1T2016+type@asset+block/pisa2009test.csv"
download.file(URL, destfile = "pisa2009train.csv",method = "curl")
download.file(URL1, destfile = "pisa2009test.csv",method = "curl")
pisaTrain <- read.csv("pisa2009train.csv")
pisaTest <- read.csv("pisa2009test.csv")
tappy(pisaTrain$readingScore, pisaTrain$male, mean)
tapply(pisaTrain$readingScore, pisaTrain$male, mean)
is.na(pisaTrain)
summary(pisaTrain)
pisaTrain = na.omit(pisaTrain)
pisaTest = na.omit(pisaTest)
summary(pisaTrain$grade)
str(pisaTrain)
pisaTrain$raceeth = relevel(pisaTrain$raceeth, "White")
pisaTest$raceeth = relevel(pisaTest$raceeth, "White")
lmScore <- lm(readingScore ~ ., data=pisaTrain)
summary(lmScore)
?predict
predictTraining <- predict(lmScore,data = pisaTrain)
rmse <- mean((predictTraining - pisaTrain$readingScore)^2)
rmse
sqrt(rmse)
Apred <- predict(lmScore,grade=9)
summary(lmScore)
10.057 * 2
29.542707 * 2
predTest <- predict(lmScore,newdata = pisaTest)
summary(predTest)
637.7 - 353.2
sqrt(mean(predTest - pisaTest$readingScore)^2)
SSE <- sum((predTest - pisaTest$readingScore)^2)
SSE
RMSE <- sqrt(SSESum/length(predTest))
RMSE <- sqrt(SSE/length(predTest))
RMSE
mean(pisaTrain$readingScore)
baseline <- 517.9629
SST <- sum((pisaTest$readingScore - baseline)^2)
SST
1 - SSE/SST
getwd()
setwd('Downloads/capstone/repo/')
list.files()
load('cleanTotalData.RData')
Data <- gsub("\\!|\\?", "", Data)
Data <- gsub("([Jj]an\\.)|([Jj]an )", "January ", Data)
Data <- gsub("([Ff]eb\\.)|([Ff]eb )", "February ", Data)
Data <- gsub("([Mm]ar\\.)|([Mm]ar )", "March ", Data)
Data <- gsub("([Aa]pr\\.)|([Aa]pr )", "Apirl ", Data)
Data <- gsub("([Mm]ay\\.)|([Mm]ay )", "May ", Data)
Data <- gsub("([Jj]un\\.)|([Jj]un )", "June ", Data)
Data <- gsub("([Jj]ul\\.)|([Jj]ul )", "July ", Data)
Data <- gsub("([Aa]ug\\.)|([Aa]ug )", "August ", Data)
Data <- gsub("([Ss]ep\\.)|([Ss]ep )", "September ", Data)
Data <- gsub("([Oo]ct\\.)|([Oo]ct )", "October ", Data)
Data <- gsub("([Nn]ov\\.)|([Nn]ov )", "November ", Data)
Data <- gsub("([Dd]ec\\.)|([Dd]ec )", "December ", Data)
Data <- gsub("\\.+$", "", Data)
Data <- gsub(" +", " ", Data)
list.files()
train_file <- file("training.txt", "r")
training <- readLines(train_file)
library(RWeka)
library(data.table)
len <- length(training)
UniTokenTab1 <- as.data.frame(table(NGramTokenizer(training[1:round(len/2)], Weka_control(min = 1, max = 1, delimiters = " \n"))))
UniTokenTab2 <- as.data.frame(table(NGramTokenizer(training[round(len/2)+1:len], Weka_control(min = 1, max = 1, delimiters = " \n"))))
head(UniTokenTab1)
tail(UniTokenTab1)
View(UniTokenTab1)
View(UniTokenTab1)
